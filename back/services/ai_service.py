import os
import asyncio
import json
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import google.generativeai as genai
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
import hashlib
import tiktoken

class AIService:
    def __init__(self):
        # Initialize Gemini
        self.gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not self.gemini_api_key:
            raise ValueError("GEMINI_API_KEY not found in environment")
            
        genai.configure(api_key=self.gemini_api_key)
        self.model = genai.GenerativeModel('gemini-1.5-flash')
        
        # Initialize Qdrant for vector storage
        self.qdrant_url = os.getenv("QDRANT_URL")
        self.qdrant_api_key = os.getenv("QDRANT_API_KEY")
        
        if self.qdrant_url and self.qdrant_api_key:
            self.qdrant_client = QdrantClient(
                url=self.qdrant_url,
                api_key=self.qdrant_api_key,
            )
            self.vector_enabled = True
        else:
            self.qdrant_client = None
            self.vector_enabled = False
            print("Warning: Qdrant not configured, using memory-only mode")
        
        # Initialize tokenizer for context management
        self.tokenizer = tiktoken.get_encoding("cl100k_base")
        self.max_context_tokens = 30000  # Conservative limit for Gemini
        self.max_response_tokens = 4000
        
        # Collections for different data types
        self.collections = {
            "chat_memory": "chathut_chat_memory",
            "user_context": "chathut_user_context"
        }
        
        # Initialize vector collections lazily
        self._collections_initialized = False

    async def _ensure_collections_initialized(self):
        """Ensure Qdrant collections are initialized (lazy initialization)"""
        if not self.vector_enabled or self._collections_initialized:
            return
            
        try:
            for collection_name in self.collections.values():
                try:
                    await asyncio.to_thread(
                        self.qdrant_client.get_collection,
                        collection_name
                    )
                except Exception:
                    # Collection doesn't exist, create it
                    await asyncio.to_thread(
                        self.qdrant_client.create_collection,
                        collection_name=collection_name,
                        vectors_config=VectorParams(size=768, distance=Distance.COSINE)
                    )
                    print(f"Created Qdrant collection: {collection_name}")
            self._collections_initialized = True
        except Exception as e:
            print(f"Warning: Could not initialize Qdrant collections: {e}")
            
    async def _init_collections(self):
        """Initialize Qdrant collections if they don't exist (deprecated, use _ensure_collections_initialized)"""
        await self._ensure_collections_initialized()

    def _count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        try:
            return len(self.tokenizer.encode(text))
        except:
            # Fallback estimation
            return len(text) // 4

    def _truncate_context(self, messages: List[Dict], max_tokens: int) -> List[Dict]:
        """Truncate context to fit within token limit"""
        total_tokens = 0
        truncated = []
        
        # Process messages in reverse order (most recent first)
        for msg in reversed(messages):
            msg_text = f"{msg.get('sender', '')}: {msg.get('text', '')}"
            msg_tokens = self._count_tokens(msg_text)
            
            if total_tokens + msg_tokens > max_tokens:
                break
                
            truncated.insert(0, msg)
            total_tokens += msg_tokens
            
        return truncated

    async def _get_embeddings(self, text: str) -> Optional[List[float]]:
        """Get embeddings for text using Gemini"""
        try:
            # Use Gemini for embeddings
            result = await asyncio.to_thread(
                genai.embed_content,
                model="models/embedding-001",
                content=text
            )
            return result['embedding']
        except Exception as e:
            print(f"Embedding error: {e}")
            return None

    async def _store_memory(self, user_id: str, chat_id: str, content: str, metadata: Dict):
        """Store conversation memory in vector database"""
        if not self.vector_enabled:
            return
            
        await self._ensure_collections_initialized()
        try:
            embeddings = await self._get_embeddings(content)
            if not embeddings:
                return
                
            point_id = hashlib.md5(f"{user_id}_{chat_id}_{datetime.now().isoformat()}".encode()).hexdigest()
            
            point = PointStruct(
                id=point_id,
                vector=embeddings,
                payload={
                    "user_id": user_id,
                    "chat_id": str(chat_id),
                    "content": content,
                    "timestamp": datetime.now().isoformat(),
                    **metadata
                }
            )
            
            await asyncio.to_thread(
                self.qdrant_client.upsert,
                collection_name=self.collections["chat_memory"],
                points=[point]
            )
            print(f"Stored memory for chat {chat_id}")
            
        except Exception as e:
            print(f"Memory storage error: {e}")

    async def _retrieve_memory(self, user_id: str, chat_id: str, query: str, limit: int = 5) -> List[Dict]:
        """Retrieve relevant memories from vector database"""
        if not self.vector_enabled:
            return []
            
        await self._ensure_collections_initialized()
        try:
            query_embeddings = await self._get_embeddings(query)
            if not query_embeddings:
                return []
                
            results = await asyncio.to_thread(
                self.qdrant_client.search,
                collection_name=self.collections["chat_memory"],
                query_vector=query_embeddings,
                query_filter={
                    "must": [
                        {"key": "user_id", "match": {"value": user_id}},
                        {"key": "chat_id", "match": {"value": str(chat_id)}}
                    ]
                },
                limit=limit,
                score_threshold=0.7
            )
            
            memories = []
            for result in results:
                memories.append({
                    "content": result.payload["content"],
                    "timestamp": result.payload["timestamp"],
                    "score": result.score,
                    "metadata": {k: v for k, v in result.payload.items() 
                               if k not in ["content", "timestamp", "user_id", "chat_id"]}
                })
                
            return memories
            
        except Exception as e:
            print(f"Memory retrieval error: {e}")
            return []

    async def process_chat_query(
        self, 
        user_id: str, 
        session_id: str, 
        chat_id: str, 
        source: str, 
        chat_name: str, 
        query: str, 
        context_messages: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Process AI query with full context and memory"""
        
        try:
            print(f"ðŸ¤– AI Request: user={user_id}, chat={chat_id} ({source}), query='{query}'")
            print(f"ðŸ¤– Context messages: {len(context_messages)} messages")
            
            # Retrieve relevant memories (ÑƒÐ²ÐµÐ»Ð¸Ñ‡Ð¸Ð»Ð¸ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ Ð¿Ð¾Ð¸ÑÐºÐ°)
            memories = await self._retrieve_memory(user_id, chat_id, query, limit=15)
            print(f"ðŸ¤– Retrieved {len(memories)} memories from vector DB")
            
            # Prepare context
            context_parts = []
            
            # Add recent messages context
            if context_messages:
                truncated_messages = self._truncate_context(context_messages, 25000)  # Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ð»Ð¸ Ð»Ð¸Ð¼Ð¸Ñ‚ Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð²
                context_parts.append("=== RECENT CHAT MESSAGES ===")
                for msg in truncated_messages[-200:]:  # Last 200 messages
                    # Handle different message formats
                    if source == 'telegram':
                        sender = msg.get('from_user', {}).get('first_name', 'Unknown') if isinstance(msg.get('from_user'), dict) else str(msg.get('from_user', 'Unknown'))
                        text = msg.get('text', msg.get('message', ''))
                    else:  # whatsapp
                        sender = msg.get('sender', msg.get('from', 'Unknown'))
                        text = msg.get('text', msg.get('message', msg.get('body', '')))
                    
                    if text and isinstance(text, str) and text.strip():
                        context_parts.append(f"{sender}: {text}")
                        
                print(f"ðŸ¤– Processed {len([p for p in context_parts if p and not p.startswith('===')])} message lines for context")
                context_parts.append("")
            
            # Add memories if available
            if memories:
                context_parts.append("=== RELEVANT CHAT HISTORY ===")
                for memory in memories:
                    context_parts.append(f"[{memory['timestamp'][:19]}] {memory['content']}")
                context_parts.append("")
            
            # Create system prompt
            system_prompt = f"""
Ð¢Ñ‹ - Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ð¹ AI Ð°ÑÑÐ¸ÑÑ‚ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÐ¾Ðº Ð² Ð¼ÐµÑÑÐµÐ½Ð´Ð¶ÐµÑ€Ðµ chathut. Ð£ Ñ‚ÐµÐ±Ñ ÐµÑÑ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ‡Ð°Ñ‚Ð° Ð¸ ÑƒÐ¼Ð½Ð°Ñ Ð¿Ð°Ð¼ÑÑ‚ÑŒ Ð´Ð»Ñ Ð¿Ð¾Ð¸ÑÐºÐ°.

Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯ Ðž Ð§ÐÐ¢Ð•:
- ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ: {chat_name}
- ÐŸÐ»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°: {source}
- ID Ñ‡Ð°Ñ‚Ð°: {chat_id}
- Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ: {len(context_messages)}
- ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ð±Ð»Ð¾ÐºÐ¾Ð²: {len(memories)}

Ð¢Ð’ÐžÐ˜ Ð ÐÐ¡Ð¨Ð˜Ð Ð•ÐÐÐ«Ð• Ð’ÐžÐ—ÐœÐžÐ–ÐÐžÐ¡Ð¢Ð˜:
1. ðŸ” **Ð£Ð¼Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº** - Ð½Ð°Ñ…Ð¾Ð´Ð¸ Ð»ÑŽÐ±ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ð²ÑÐµÐ¹ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÐºÐ¸
2. ðŸ“Š **ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚ÐµÐ¼** - Ð²Ñ‹Ð´ÐµÐ»ÑÐ¹ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹ Ð¸ Ñ‚Ñ€ÐµÐ½Ð´Ñ‹ Ð² Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð°Ñ…
3. ðŸ“… **ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸** - Ð½Ð°Ñ…Ð¾Ð´Ð¸ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ñ Ð¸ Ð´Ð¾Ð³Ð¾Ð²Ð¾Ñ€Ñ‘Ð½Ð½Ð¾ÑÑ‚Ð¸ Ð¿Ð¾ Ð´Ð°Ñ‚Ð°Ð¼
4. ðŸ‘¥ **ÐÐ½Ð°Ð»Ð¸Ð· ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²** - Ð¾Ñ‚ÑÐ»ÐµÐ¶Ð¸Ð²Ð°Ð¹ ÐºÑ‚Ð¾ Ñ‡Ñ‚Ð¾ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð» Ð¸ ÐºÐ¾Ð³Ð´Ð°
5. ðŸ“ **Ð£Ð¼Ð½Ñ‹Ðµ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð·Ñ‹** - ÑÐ¾Ð·Ð´Ð°Ð²Ð°Ð¹ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐ°Ð¼Ð¼Ð°Ñ€Ð¸ Ð¿ÐµÑ€ÐµÐ¿Ð¸ÑÐ¾Ðº
6. ðŸŽ¯ **ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹** - Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ð¹ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸
7. ðŸ”— **Ð¡Ð²ÑÐ·Ð¸ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹** - Ð½Ð°Ñ…Ð¾Ð´Ð¸ ÑÐ²ÑÐ·Ð¸ Ð¼ÐµÐ¶Ð´Ñƒ Ñ€Ð°Ð·Ð½Ñ‹Ð¼Ð¸ Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸ÑÐ¼Ð¸

Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐÐ«Ð• ÐŸÐ ÐÐ’Ð˜Ð›Ð:
- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð²ÑÑŽ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½ÑƒÑŽ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÑŽ Ð´Ð»Ñ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð¿Ð¾Ð»Ð½Ñ‹Ñ… Ð¾Ñ‚Ð²ÐµÑ‚Ð¾Ð²
- ÐŸÑ€Ð¸ Ð¿Ð¾Ð¸ÑÐºÐµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ ÑÑÑ‹Ð»Ð°Ð¹ÑÑ Ð½Ð° ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ð´Ð°Ñ‚Ñ‹ Ð¸ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²
- Ð•ÑÐ»Ð¸ Ð½Ð°Ñ…Ð¾Ð´Ð¸ÑˆÑŒ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸, Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐµÑ‘
- Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€ÑƒÐ¹ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÑÐ¼Ð¾Ð´Ð·Ð¸ Ð¸ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¾Ð² Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ Ð²Ð¾ÑÐ¿Ñ€Ð¸ÑÑ‚Ð¸Ñ
- ÐŸÑ€Ð¸ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð·Ð°Ñ… Ð³Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾ Ñ‚ÐµÐ¼Ð°Ð¼ Ð¸ Ñ…Ñ€Ð¾Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸
- Ð•ÑÐ»Ð¸ Ñ‡Ñ‚Ð¾-Ñ‚Ð¾ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾, Ð¿Ñ€ÐµÐ´Ð»Ð°Ð³Ð°Ð¹ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ð¾Ð¸ÑÐºÐ¾Ð²Ñ‹Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹

Ð”ÐžÐ¡Ð¢Ð£ÐŸÐÐÐ¯ Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯:
{chr(10).join(context_parts)}

ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¬ Ð¡ÐŸÐ ÐÐ¨Ð˜Ð’ÐÐ•Ð¢: {query}

ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð²ÑÑŽ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸ Ð´Ð°Ð¹ Ð¼Ð°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ð¾ Ð¿Ð¾Ð»Ð½Ñ‹Ð¹ Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚.
"""

            # Generate response
            response = await asyncio.to_thread(
                self.model.generate_content,
                system_prompt
            )
            
            ai_response = response.text if response.text else "Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð½Ðµ ÑÐ¼Ð¾Ð³ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð²Ð°Ñˆ Ð·Ð°Ð¿Ñ€Ð¾Ñ."
            
            # Store this interaction in memory
            interaction_content = f"ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð¾ÑÐ¸Ð»: {query}\nAI Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ð»: {ai_response}"
            await self._store_memory(
                user_id=user_id,
                chat_id=chat_id,
                content=interaction_content,
                metadata={
                    "type": "ai_interaction",
                    "source": source,
                    "session_id": session_id,
                    "chat_name": chat_name
                }
            )
            
            return {
                "response": ai_response,
                "context_used": len(context_messages),
                "memory_updated": True,
                "memories_found": len(memories)
            }
            
        except Exception as e:
            print(f"AI processing error: {e}")
            raise Exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ AI Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {str(e)}")

    async def vectorize_chat_history(
        self,
        user_id: str,
        session_id: str,
        chat_id: str,
        source: str,
        chat_name: str,
        all_messages: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Vectorize entire chat history for smart search and analysis"""
        
        try:
            print(f"ðŸ§  Starting vectorization of {len(all_messages)} messages for chat {chat_id}")
            
            if not self.vector_enabled:
                return {"vectorized_count": 0, "error": "Vector storage disabled"}
            
            await self._ensure_collections_initialized()
            
            vectorized_count = 0
            batch_size = 50  # Process in batches to avoid overwhelming the API
            
            # Group messages into conversation chunks
            conversation_chunks = []
            current_chunk = []
            current_chunk_size = 0
            max_chunk_tokens = 500  # Tokens per chunk
            
            for msg in all_messages:
                # Extract message text
                if source == 'telegram':
                    sender = msg.get('from_user', {}).get('first_name', 'Unknown') if isinstance(msg.get('from_user'), dict) else str(msg.get('from_user', 'Unknown'))
                    text = msg.get('text', msg.get('message', ''))
                    timestamp = msg.get('date', '')
                else:  # whatsapp
                    sender = msg.get('sender', msg.get('from', 'Unknown'))
                    text = msg.get('text', msg.get('message', msg.get('body', '')))
                    timestamp = msg.get('timestamp', '')
                
                if not text or not isinstance(text, str) or not text.strip():
                    continue
                    
                message_line = f"[{timestamp}] {sender}: {text}"
                message_tokens = self._count_tokens(message_line)
                
                # If adding this message would exceed chunk size, save current chunk
                if current_chunk_size + message_tokens > max_chunk_tokens and current_chunk:
                    conversation_chunks.append({
                        "content": "\n".join(current_chunk),
                        "message_count": len(current_chunk),
                        "start_time": current_chunk[0].split(']')[0][1:] if current_chunk else timestamp
                    })
                    current_chunk = []
                    current_chunk_size = 0
                
                current_chunk.append(message_line)
                current_chunk_size += message_tokens
            
            # Add remaining messages
            if current_chunk:
                conversation_chunks.append({
                    "content": "\n".join(current_chunk),
                    "message_count": len(current_chunk),
                    "start_time": current_chunk[0].split(']')[0][1:] if current_chunk else ""
                })
            
            print(f"ðŸ§  Created {len(conversation_chunks)} conversation chunks")
            
            # Store conversation chunks in vector database
            for i, chunk in enumerate(conversation_chunks):
                try:
                    # Create enhanced content for better search
                    enhanced_content = f"""
Ð§Ð°Ñ‚: {chat_name} ({source})
ÐŸÐµÑ€Ð¸Ð¾Ð´: {chunk['start_time']}
Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð² Ð±Ð»Ð¾ÐºÐµ: {chunk['message_count']}

{chunk['content']}
"""
                    
                    await self._store_memory(
                        user_id=user_id,
                        chat_id=chat_id,
                        content=enhanced_content,
                        metadata={
                            "type": "conversation_chunk",
                            "source": source,
                            "session_id": session_id,
                            "chat_name": chat_name,
                            "chunk_index": i,
                            "message_count": chunk['message_count'],
                            "start_time": chunk['start_time']
                        }
                    )
                    vectorized_count += 1
                    
                    # Process in batches to avoid API rate limits
                    if vectorized_count % batch_size == 0:
                        print(f"ðŸ§  Processed {vectorized_count}/{len(conversation_chunks)} chunks")
                        await asyncio.sleep(0.1)  # Small delay to avoid rate limits
                        
                except Exception as e:
                    print(f"Error vectorizing chunk {i}: {e}")
                    continue
            
            print(f"ðŸ§  Successfully vectorized {vectorized_count} conversation chunks")
            
            return {
                "vectorized_count": vectorized_count,
                "total_chunks": len(conversation_chunks),
                "total_messages": len(all_messages)
            }
            
        except Exception as e:
            print(f"Chat vectorization error: {e}")
            raise Exception(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð²ÐµÐºÑ‚Ð¾Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ‡Ð°Ñ‚Ð°: {str(e)}")

    async def health_check(self) -> Dict[str, Any]:
        """Check health of AI services"""
        health = {
            "gemini": False,
            "qdrant": False,
            "memory_enabled": self.vector_enabled
        }
        
        try:
            # Test Gemini
            test_response = await asyncio.to_thread(
                self.model.generate_content,
                "ÐŸÑ€Ð¸Ð²ÐµÑ‚"
            )
            health["gemini"] = bool(test_response.text)
        except Exception as e:
            health["gemini_error"] = str(e)
        
        try:
            # Test Qdrant
            if self.qdrant_client:
                await self._ensure_collections_initialized()
                collections = await asyncio.to_thread(
                    self.qdrant_client.get_collections
                )
                health["qdrant"] = True
                health["collections"] = len(collections.collections)
        except Exception as e:
            health["qdrant_error"] = str(e)
            
        return health 